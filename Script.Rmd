---
title: "Script"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tabulizer)
library(dplyr)
library(stringr)
library(ggplot2)
library(fUnitRoots)
library(forecast)
library(lmtest)
library(prophet)
```

## Obtener datos desde el PDF
```{r}
pages<-extract_tables("C01-Importación-de-combustibles-VOLUMEN-2020-03.pdf") #PDF con datos actualizados
datosImp <- do.call(rbind, pages)
nombresVar<-datosImp[1,]
datosImp<-as.data.frame(datosImp[2:nrow(datosImp),])
nombresVar[c(1,4,5,6,8,10,11,15,16,21,23,24)]<-c("Anio","GasAviacion","GasSuperior","GasRegular","rTurboJet","DieselLS","DieselULS","AceitesLub","GrasasLub","PetroleoReconst","Orimulsion","MezclasOleosas")
names(datosImp)<-nombresVar
```

## Leemos los datos de un CSV
```{r}
#Leer
dataSet = read.csv("datos.csv",stringsAsFactors = FALSE, na.strings = TRUE, strip.white = TRUE,sep = ",", encoding="UTF-8" )

```


## Limpiamos la data
```{r}
#Seleccionar
dataset = dataSet[c(1,2,5,6, 9,10)]
dataset = dataset[-c(46, 96, 146, 196),]
```

```{r}
#Limpiar
dataset$Diesel[dataset$Diesel =="-"] <- 0
dataset$DieselLS[dataset$DieselLS =="-"] <- 0
dataset2 = dataset
```

## Implicamos el dataset limpio
```{r}
#COnvertir
options(digits=9)
dataset2$Anio = as.numeric(dataset2$Anio)
dataset2$Mes = as.numeric(dataset2$Mes)
dataset2$GasSuperior = as.numeric(gsub(",", "", dataset2$GasSuperior))
dataset2$GasRegular = as.numeric(gsub(",", "", dataset2$GasRegular))
dataset2$Diesel = as.numeric(gsub(",", "", dataset2$Diesel))
dataset2$DieselLS = as.numeric(gsub(",", "", dataset2$DieselLS))
#Unir Diesel
dataset2$Diesel = dataset2$Diesel + dataset2$DieselLS
dataset2 = dataset2[-6]
dataSet = dataset2
```

### Clasifición de las variables

##ANIO

Año del índice

Variable categórica ordinal

##Mes

Mes del índice

Variable categórica ordinal

##GasSuperior

Volumen de importación gasolina superior

Variable cuantitativa continua

```{r}
qqnorm(dataSet$GasSuperior, main = "Distribucion normal de $GasSuperior")
qqline(dataSet$GasSuperior)
```

Por la gráfica anterior podemos afirmar que la varaible cuenta con una distribución normal. 

##GasRegular

Volumen de importación gasolina regular

Variable cuantitativa continua

```{r}
qqnorm(dataSet$GasSuperior, main = "Distribucion normal de $GasRegular")
qqline(dataSet$GasSuperior)
```

Por la gráfica anterior podemos afirmar que la varaible cuenta con una distribución normal. 

##Diesel

Volumen de importación diesel

Variable cuantitativa continua

```{r}
qqnorm(dataSet$GasSuperior, main = "Distribucion normal de $Diesel")
qqline(dataSet$GasSuperior)
```
Por la gráfica anterior podemos afirmar que la varaible cuenta con una distribución normal. 

## Preguntas

### En que año fue mayor la importación de diesel promedio?
```{r}
p <- dataSet %>% 
group_by(Anio) %>% 
summarise(mean = mean(Diesel)) 
p[order(p$mean),]
```

Como podemos observar existe una tendencia de crecimiento en la importacion de diesel. Es interesante ver el decremento que existió en el año 2018 y 2017, gracias a que rompe esta tendencia. De igual manera 2009, es un pico en la importacion que se comporta de manera atípica. 


### En que año fue mayor la importación de gasolina regular  promedio?
```{r}
p <- dataSet %>% 
group_by(Anio) %>% 
summarise(mean = mean(GasRegular)) 
p[order(p$mean),]
```
Como podemos observar existe una tendencia de crecimiento en la importacion de gasolina. Es interesante ver que el 2009, es un pico en la importacion que se comporta de manera atípica. 

### En que año fue mayor la importación de gasolina superior promedio?
```{r}
p <- dataSet %>% 
group_by(Anio) %>% 
summarise(mean = mean(GasSuperior)) 
p[order(p$mean),]
```
Como podemos observar existe una tendencia de crecimiento en la importacion de gasolina. Es interesante ver el decremento que existio en el ano 2018, gracias a que rompe esta tendencia. De igual manera 2007, es un pico en la importacion que se comporta de manera atípica. 

### En que mes es mayor la importación de diesel promedio?
```{r}
p <- dataSet %>% 
group_by(Mes) %>% 
summarise(mean = mean(Diesel)) 
p[order(p$mean),]
```
Como podemos observar marzo es el mes en donde es mayor el consumo de diesel, seguido por diciembre.  Y septiembre es el mes en donde menos se consume.

### En que mes es mayor la importación de gasolina regular promedio?
```{r}
p <- dataSet %>% 
group_by(Mes) %>% 
summarise(mean = mean(GasRegular)) 
p[order(p$mean),]
```

Como podemos observar marzo es el mes en donde es mayor el consumo de gasolina, seguido por enero. Y septiembre es el mes en donde menos se consume. Es interesante ver como diciembre se encuentra en la mitad de la tabla, en específico para la gasolina regular. 

### En que mes es mayor la importación de gasolina superior promedio?
```{r}
p <- dataSet %>% 
group_by(Mes) %>% 
summarise(mean = mean(GasSuperior)) 
p[order(p$mean),]
```
Como podemos observar marzo es el mes en donde es mayor el consumo de gasolina, seguido por diciembre. Y septiembre es el mes en donde menos se consume. Es interesante ver los picos en al principio y fin de año, gracias a que en los cuartos de mitad de año se encuentra los puntos más bajos. 

## Serie de tiempo Gasolina Regular

Primero creamos la serie de tiempo para los datos del Diesel
```{r}
#View(dataSet)

regular <- ts(dataSet$GasRegular, start=c(2001, 1), end=c(2020,3), frequency=12)

start(regular)
end(regular)

```

## Construcción del modelo ARIMA GAS REGULAR

### Identificación
A continuación se exploran las características de la serie: 

Frecuencia de la serie: 
```{r}
frequency(regular)
```

Gráfica de la serie de tiempo
```{r}
plot(regular)
abline(reg=lm(regular~time(regular)), col=c("red"))
```

Podemos observar que es una serie de tiempo continua con tendencia a crecer por lo que es una serie de tiempo no estacionaria. 

Descomposición de la serie
```{r}
plot(decompose(regular))
```

Podemos observar una serie con tendencia a aumentar, que no es estacionaria en varianza y también tiene estacionalidad.


A continuación se separa la serie de tiempo en entrenamiento y prueba

```{r}
train <- head(regular, round(length(regular) * 0.7))
h <- length(regular) - length(train)
test <- tail(regular, h)
```

## Estimar los parámetros del modelo
A continuación aplicaremos una transformación logaritmica para hacer que la serie sea constante en varianza. 

```{r}
logTrain <- log(regular)
plot(decompose(logTrain))
```

```{r}
plot(logTrain)
abline(reg=lm(logTrain~time(logTrain)), col=c("red"))
```

Podemos notar que se logro hacer un poco más constante la varianza de la serie. A continuación verificaremos que es estacionaria en media. Si tiene raices unitarias podemos decir que no es estacionaria en media. 

```{r}
adfTest(train)
```

```{r}
unitrootTest(train)
```

Como podemos notar, en ambas pruebas el valor de p es mayor a 0.05 por lo que no se puede rechazar la hipótesis nula de ausencia de raíces unitarias. Entonces por ende no es estacionaria en media.  

A coninuación hacemos lo mismo pero con una diferenciación:

```{r}
adfTest(diff(train))
```

```{r}
unitrootTest(diff(train))
```

Como podemos observar en esta ocasión el valor de p está por debajo de 0.05 por lo que ahora si se puede descartar la hipótesis nula de que existen raíces unitarias. Podemos notar entonces que solo es necesaria una diferenciación (d=1). 

El siguiente paso es intentar identificar los parámetros p y q usando los gráficos de autocorrelación y autocorrelación parcial.

```{r}
acf(logTrain,100) 
```

En el gráfico de la función podemos notar que se anula después del quinto retardo por lo que se puede sugerir q=5.

```{r}
pacf(logTrain,20) 
```

En el gráfico de la función podemos notar que se anula después del retardo 0 por lo que podríamos sugerir un coeficiente p=0.

Podemos sugerir un modelo con los siguientes parámetros: 

* p=0.
* q=5.
* d=1.

O en otras palabras ARIMA(p,d,q).

Volvamos a ver la descomposición de la serie paran determinar si hay estacionalidad 

```{r}
decTrain <- decompose(logTrain)
plot(decTrain$seasonal)
```

Podemos notar que si existe estacionalidad en la serie. A continuación, se buscarán los parámetros estacionales usando las funciones de autocorrelación y autocorrelación parcial. 

```{r}
Acf(diff(logTrain),84)
```

Podemos notar que en casi todos los períodos hay dos decaimientos estacionales por lo que podemos sugerir P=2.

```{r}
Pacf(diff(logTrain),60)
```

Podemos darnos cuenta que los pico significativos están al inicio de la gráfica por lo que podemos sugerir D=1. Finalmente Q=0.

Con los parámetros determinados generamos un modelo: 

```{r}
fitArima <- arima(logTrain,order=c(0,1,5),seasonal = c(2,1,0))
```

R tiene la capacidad de generar un modelo automáticamente, entonces también lo tomaremos en cuenta: 

```{r}
fitAutoArima <- auto.arima(train)
```


## Significación de los coeficientes: 

A continuación, analizaremos que tan significativos son los coeficientes de cada modelo

```{r}
coeftest(fitArima)
```

En este caso podemos notar no todos los coeficientes son significativos.

```{r}
coeftest(fitAutoArima)
```

Podemos notar que todos los coeficientes son significativo ya que todos son menores a 0.05.

## Análisis de Residuales

A continuación, se analizarán los residuales de ambos modelos. Estos deben estar distribuidos normalmente y se deben de parecer al ruido blanco.

```{r}
qqnorm(fitArima$residuals)
qqline(fitArima$residuals)
```

```{r}
checkresiduals(fitArima)
```

Podemos notar que los datos tienen una distribución normal. El test de Ljung-Box muestra que  los datos se distribuyen de forma independiente puesto que el p-value es mayor a 0.05 y por lo que no se puede rechazar la hipótesis nula. Esto quiere decir que el modelo es aceptable para predecir.

Analicemos también el modelo generado automáticamente por R: 

```{r}
qqnorm(fitAutoArima$residuals)
qqline(fitAutoArima$residuals)
```
```{r}
checkresiduals(fitAutoArima)
```
Podemos notar que los datos tienen una distribución normal. Sin embargo, según el test de Ljung-Box los datos se distribuyen de forma dependiente puesto que el p-value es menor a 0.05 y se puede rechazar la hipótesis nula. Esto quiere decir que el modelo no es aceptable para predecir.

## Predicción con el modelo generado

Pasaremos a predecir con la función forecast la misma cantidad de meses que hay en test y compararemos con los valores reales. 

```{r}
fitArima <- arima(log(train),c(0,1,3),seasonal =list( order=c(2,1,0),period=12))
test2<-ts(test,start = c(2014,7),end = c(2020,3),frequency = 12)
fitArima %>%
  forecast(h) %>%
  autoplot() + autolayer(log(test2))
```

Podemos notar que nuestro modelo es bueno prediciendo, ya que las predicciones están dentro del intervalo de confianza.

```{r}
fitAutoArima <- auto.arima(train)
test3<-ts(test,start = c(2014,7),end = c(2020,3),frequency = 12)
fitAutoArima %>%
  forecast(h) %>%
  autoplot() + autolayer(test3)
```

Podemos notar que el modelo realizado automáticamente por R no predice muy bien, ya que no se parece en nada a los datos reales. Entonces podemos decir que nosotros hemos construido un mejor modelo al que genera automáticamente R. 

## Construcción del modelo Prophet

Preparamos los datos como los necesita este algoritmo: 
```{r}
df<-data.frame(ds=as.Date(as.yearmon(time(train))),y=as.matrix(train) )
testdf<-data.frame(ds=as.Date(as.yearmon(time(test))),y=as.matrix(test) )
```

A continuación, elaboramos el modelo: 

```{r}
fitProphet<-prophet(df,yearly.seasonality = T,weekly.seasonality = T)

```

Usaremos el modelo para predecir la misma cantidad de meses que el modelo ARIMA: 

```{r}
future <- make_future_dataframe(fitProphet,periods = h,freq = "month", include_history = T)
p <- predict(fitProphet,future)
p<-p[,c("ds","yhat","yhat_lower","yhat_upper")]
plot(fitProphet,p)
```


```{r}
pred<-tail(p,h)
pred$y<-testdf$y

ggplot(pred, aes(x=ds, y=yhat)) +
   geom_line(size=1, alpha=0.8) +
   geom_ribbon(aes(ymin=yhat_lower, ymax=yhat_upper), fill="blue", alpha=0.2) +
   geom_line(data=pred, aes(x=ds, y=y),color="red")
```

Podemos notar que el modelo Prophet no es muy bueno, por que no se encuentra en los intervalos de confianza. En comparación con el modelo que realizamos ARIMA , nuestro predicción se parece más a los datos reales por lo que podemos decir que el modelo ARIMA es mejor.

## Construcción del modelo ARIMA GAS REGULAR

Creamos la serie de tiempo
```{r}
superior <- ts(dataSet$GasSuperior, start=c(2001, 1), end=c(2020, 3), frequency=12)
```

Establecemos datos a usar
```{r}
train <- head(superior, round(length(superior) * 0.7))
h <- length(superior) - length(train)
test <- tail(superior, h)
```

### Iniciamos la construcción del modelo

Primero, analizaremos el gráfico de la serie
```{r}
plot(superior)
abline(reg=lm(superior~time(superior)), col=c("red"))
```

Es una serie con frecuencia anual desde enero 2001 hasta marzo 2020. La línea de la media nos indica que la serie tiene una tendencia a crecer. Cabe destacar que, durante los años 2011 y 2015 el precio se mantuvo estable, es hasta después del 2015 que el precio creció de nuevo.

### Descomposición de la serie
```{r}
dec.superior<-decompose(superior)
plot(dec.superior)
```

La tendencia no es estacionaria en varianza, pues los rangos varían bastante con el tiempo. La serie tiene estacionalidad.

### Estimar parámetros del modelo
Dado que no es estacionaria en varianza le aplicaremos una transformación logaritmica para hacerla constante en varianza. Lo haremos con la serie de entrenamiento que es la que nos ayudará a predecir
```{r}
logTrain <- log(superior)
```

Intento de estacionalizar la serie con una transformación logarítmica.
```{r}
plot(decompose(logTrain))

plot(logTrain)
abline(reg=lm(logTrain~time(logTrain)), col=c("red"))
```

Al parecer se logra mejorar que la varianza sea un poco más constante. Debemos verificar si es estacionaria en media. Si tiene raices unitarias podemos decir que no es estacionaria en media y hay que aplicar procesos de diferenciación. Uno de los factores que nos llevan a pensar que en efecto la varianza es un poco más constante es que la gráfica posee menos oscilación.


### Usando la prueba de Dickey-Fuller
```{r}
adfTest(train)
```
El valor p es mayor a 0.05, por lo que no se puede rechazar la hipótesis nula de las raices unitarias.

### Segunda prueba
```{r}
unitrootTest(train)
```
El valor p tampoco es mayor a 0.05. Esto significa que la serie no es estacionaria en media.


### Aplicando diferenciación
Se aplicará una diferenciación con el fin de hacer la serie estacionario en media.
```{r}
adfTest(diff(train))
unitrootTest(diff(train))
```
En ambas pruebas el valor p es menor a 0.05. Con una sola diferenciación se puede rechazar la hipótesis nula de las raíces unitarias. Por lo tanto:

- d = 1

### Identificando parámetros p y q
Función de autocorrelación:
```{r}
acf(logTrain,80)
```

Con respecto al presente gráfico, podemos ver que se anula el valor j se anula luego del tercer retardo.

- Se propone un valor q = 3


### Función de correlación parcial
```{r}
pacf(logTrain,80)
```
Se anula luego del segundo retardo, por ello:

- Se propone un valor p = 2

### Resumen de parámetros:
- d = 1
- p = 2
- q = 3
- ARIMA(2,1,3)

### ¿Estacionalidad en la serie?
```{r}
decTrain <- decompose(superior)
plot(decTrain$seasonal)
```

A simple vista, parece que la serie sí tiene estacionalidad. 


### Uso de la función de autocorrelación
Se usará la serie estacionarizada.
```{r}
Acf(diff(logTrain),84)
```

Los períodos poseen 2 decaimientos estacionales. Por ello:

- P = 2

```{r}
Pacf(diff(logTrain),84)
```

Los picos significativos están durante el primer período. Por ello:

- Q = 1
- D = 0

### Generación del modelo

Modelo con parámetros obtenidos y modelo automático, respectivamente:
```{r}
fitArima <- arima(logTrain,order=c(2,1,3),seasonal = c(2,1,0))
fitAutoArima <- auto.arima(train)
```

Validamos si los coeficientes son significativos:
```{r}
coeftest(fitArima)
```

Todos son menores a 0.05, por lo que sí son significativos

###  Análisis de residuales

```{r}
qqnorm(fitArima$residuals)
qqline(fitArima$residuals)
```

```{r}
checkresiduals(fitArima)
```

Los residuales parecen seguir una distribución normal. Los datos se distribuyen de forma dependiente puesto que el valor p es menor a 0.05. En efecto, se puede rechazar la hipótesis nula, es decir, el modelo **no** es aceptable para predecir.


###  Análisis de residuales del modelo automático
```{r}
qqnorm(fitAutoArima$residuals)
qqline(fitAutoArima$residuals)
```
```{r}
checkresiduals(fitAutoArima)
```

El test Ljung-Box dice que los datos se distribuyen de forma independiente puesto que el valor-p es mayor a 0.05. Puede rechazarse la hipótesis nula, el modelo es aceptable para predecir.


### Predicción con modelo generado
```{r}
fitArima %>%
  forecast(h) %>%
  autoplot() + autolayer(log(test))
```

#### Interpretación:
El modelo no llega a tener una predicción totalmente acertada, sin embargo, una gran parte del valor predecido entra en el área segura de predicción. Esto quiere decir que el modelo que hemos generado sí tiene la capacidad de predecir adecuadamente el comportamiento del gas superior, aunque no de una forma totalmente viable. Parte de esto puede deberse a que el modelo estuvo expuesto a un crecimiento constante por parte de los precios de la gasolina, sin embargo, a partir del 2015 los precios se mantuvieron casi al mismo nivel que los 8 años anteriores y eso causó una predicción no acertada.

### Predicción con el modelo automatico
```{r}
testTs<-ts(test,start = c(2014,7),end = c(2020,3),frequency = 12)

fitAutoArima %>%
  forecast(h) %>%
  autoplot() + autolayer(testTs)
```

El modelo generado por automáticamente no logró predecir adecuadamente. Muchas de sus predicciones no entran en el área segura aunque sí parece tener un comportamiento más constante.


### Conclusión del modelo GAS SUPERIOR: 

- Se logró obtener un modelo generado que predijera de forma considerablemente certera los datos reales.
- El modelo generado automáticamente por R no obtuvo resultados tan eficientes como en análisis de los comportamientos y la obtención de parámetros manuales.

## Modelo de Diesel


```{r}
#Leer
dataSet = read.csv("datos.csv",stringsAsFactors = FALSE, na.strings = TRUE, strip.white = TRUE,sep = ",", encoding="UTF-8" )

```

```{r}
#Seleccionar
dataset = dataSet[c(1,2,5,6, 9,10)]
dataset = dataset[-c(46, 96, 146, 196),]
```

```{r}
#Limpiar
dataset$Diesel[dataset$Diesel =="-"] <- 0
dataset$DieselLS[dataset$DieselLS =="-"] <- 0
dataset2 = dataset
```

```{r}
#COnvertir
options(digits=9)
dataset2$Anio = as.numeric(dataset2$Anio)
dataset2$Mes = as.numeric(dataset2$Mes)
dataset2$GasSuperior = as.numeric(gsub(",", "", dataset2$GasSuperior))
dataset2$GasRegular = as.numeric(gsub(",", "", dataset2$GasRegular))
dataset2$Diesel = as.numeric(gsub(",", "", dataset2$Diesel))
dataset2$DieselLS = as.numeric(gsub(",", "", dataset2$DieselLS))
#Unir Diesel
dataset2$Diesel = dataset2$Diesel + dataset2$DieselLS
dataset2 = dataset2[-6]
dataSet = dataset2
View(dataSet)
```



## Serie de tiempo Diesel

Primero creamos la serie de tiempo para los datos del Diesel
```{r}
View(dataSet)
diesel <- ts(dataSet[dataSet$Diesel!="Diesel" & dataSet$Diesel!="-","Diesel"], start=c(2001, 1), end=c(2020,3), frequency=12)

class(diesel)
View(diesel)
```

## Construcción del modelo ARIMA

# Identificación 
A continuación se exploran las características de la serie: 

Frecuencia de la serie: 
```{r}
frequency(diesel)
```

Frecuencia de la serie: 

# Gráfica de la serie de tiempo
```{r}
plot(diesel)
abline(reg=lm(diesel~time(diesel)), col=c("red"))
```

Podemos observar que es una serie de tiempo continua con ligera tendencia a crecer por lo que es una serie de tiempo no estacionaria. 


# Descomposición de la serie
```{r}
plot(decompose(diesel))
```

Podemos observar una serie con tendencia a aumentar, que no es estacionaria en varianza y también tiene estacionalidad.


A continuación se separa la serie de tiempo en entrenamiento y prueba

```{r}
train <- head(diesel, round(length(diesel) * 0.7))
h <- length(diesel) - length(train)
test <- tail(diesel, h)
```

## Estimar los parámetros del modelo
A continuación aplicaremos una transformación logaritmica para hacer que la serie sea constante en varianza. 

```{r}
logTrain <- log(diesel)
plot(decompose(logTrain))
```

```{r}
plot(logTrain)
abline(reg=lm(logTrain~time(logTrain)), col=c("red"))
```

Podemos notar que se logro hacer un poco más constante la varianza de la serie. A continuación verificaremos que es estacionaria en media. Si tiene raices unitarias podemos decir que no es estacionaria en media. 

```{r}
adfTest(train)
```

```{r}
unitrootTest(train)
```

Como podemos notar, en ambas pruebas el valor de p es mayor a 0.05 por lo que no se puede rechazar la hipótesis nula de ausencia de raíces unitarias. Entonces por ende no es estacionaria en media.  

A coninuación hacemos lo mismo pero con una diferenciación:

```{r}
adfTest(diff(train))
```

```{r}
unitrootTest(diff(train))

```

Como podemos observar en esta ocasión el valor de p está por debajo de 0.05 por lo que ahora si se puede descartar la hipótesis nula de que existen raíces unitarias. Podemos notar entonces que solo es necesaria una diferenciación (d=1). 

El siguiente paso es intentar identificar los parámetros p y q usando los gráficos de autocorrelación y autocorrelación parcial.

```{r}
acf(logTrain,50) 
```

En el gráfico de la función podemos notar que se anula después del tercer retardo por lo que se puede sugerir q=3.

```{r}
pacf(logTrain,20) 
```

En el gráfico de la función podemos notar que se anula después del retardo 0 por lo que podríamos sugerir un coeficiente p=0.

Podemos sugerir un modelo con los siguientes parámetros: 

* p=0.
* q=3.
* d=1.

O en otras palabras ARIMA(p,d,q).

Volvamos a ver la descomposición de la serie paran determinar si hay estacionalidad 

```{r}
decTrain <- decompose(logTrain)
plot(decTrain$seasonal)
```

Podemos notar que si existe estacionalidad en la serie. A continuación, se buscarán los parámetros estacionales usando las funciones de autocorrelación y autocorrelación parcial. 

```{r}
Acf(diff(logTrain),84)
```

Podemos notar que en casi todos los períodos hay dos decaimientos estacionales por lo que podemos sugerir P=2.

```{r}
Pacf(diff(logTrain),84)
```

Podemos darnos cuenta que los pico significativos están al inicio de la gráfica por lo que podemos sugerir D=1. Finalmente Q=0.

Con los parámetros determinados generamos un modelo: 

```{r}
fitArima <- arima(logTrain,order=c(0,1,3),seasonal = c(2,1,0))
```

R tiene la capacidad de generar un modelo automáticamente, entonces también lo tomaremos en cuenta: 

```{r}
fitAutoArima <- auto.arima(train)
```


## Significación de los coeficientes: 

A continuación, analizaremos que tan significativos son los coeficientes de cada modelo

```{r}
coeftest(fitArima)
```

Podemos notar que todos los coeficientes son significativo ya que todos son menores a 0.05.

```{r}
coeftest(fitAutoArima)
```

En este caso podemos notar no todos los coeficientes son significativos. 

## Análisis de Residuales

A continuación, se analizarán los residuales de ambos modelos. Estos deben estar distribuidos normalmente y se deben de parecer al ruido blanco.

```{r}
qqnorm(fitArima$residuals)
qqline(fitArima$residuals)
```

```{r}
checkresiduals(fitArima)
```

Podemos notar que los datos tienen una distribución normal. Sin embargo, según el test de Ljung-Box los datos se distribuyen de forma dependiente puesto que el p-value es menor a 0.05 y se puede rechazar la hipótesis nula. Esto quiere decir que el modelo no es aceptable para predecir.

Analicemos también el modelo generado automáticamente por R: 

```{r}
qqnorm(fitAutoArima$residuals)
qqline(fitAutoArima$residuals)
```
```{r}
checkresiduals(fitAutoArima)
```

En este caso podemos notar que el test Ljung-Box dice que los datos se distribuyen de forma independiente puesto que el p-value es mayor a 0.05, por lo tanto, puede rechazarse la hipótesis nula y el modelo es aceptable para predecir. 


## Predicción con el modelo generado

Pasaremos a predecir con la función forecast la misma cantidad de meses que hay en test y compararemos con los valores reales. 

```{r}
fitArima <- arima(log(train),c(0,1,3),seasonal =list( order=c(2,1,0),period=12))
test2<-ts(test,start = c(2014,7),end = c(2020,3),frequency = 12)
fitArima %>%
  forecast(h) %>%
  autoplot() + autolayer(log(test2))
```

Podemos notar que nuestro modelo es bueno prediciendo, ya que este se parece bastante a los datos reales, así como también está dentro del intervalo de confianza.

```{r}
fitAutoArima <- auto.arima(train)
test3<-ts(test,start = c(2014,7),end = c(2020,3),frequency = 12)
fitAutoArima %>%
  forecast(h) %>%
  autoplot() + autolayer(test3)
```

Podemos notar que el modelo realizado automáticamente por R no predice muy bien, ya que no se parece en nada a los datos reales. Entonces podemos decir que nosotros hemos construido un mejor modelo al que genera automáticamente R. 



## Construcción del modelo Prophet

Preparamos los datos como los necesita este algoritmo: 
```{r}
df<-data.frame(ds=as.Date(as.yearmon(time(train))),y=as.matrix(train) )
testdf<-data.frame(ds=as.Date(as.yearmon(time(test))),y=as.matrix(test) )
```

A continuación, elaboramos el modelo: 

```{r}
fitProphet<-prophet(df,yearly.seasonality = T,weekly.seasonality = T)

```

Usaremos el modelo para predecir la misma cantidad de meses que el modelo ARIMA: 

```{r}
future <- make_future_dataframe(fitProphet,periods = h,freq = "month", include_history = T)
p <- predict(fitProphet,future)
p<-p[,c("ds","yhat","yhat_lower","yhat_upper")]
plot(fitProphet,p)
```


```{r}
pred<-tail(p,h)
pred$y<-testdf$y

ggplot(pred, aes(x=ds, y=yhat)) +
   geom_line(size=1, alpha=0.8) +
   geom_ribbon(aes(ymin=yhat_lower, ymax=yhat_upper), fill="blue", alpha=0.2) +
   geom_line(data=pred, aes(x=ds, y=y),color="red")
```

Podemos notar que el modelo Prophet es considerablemente bueno aunque se sale un poco de los intervalos de confianza, sin embargo si hacemos una comparación con el modelo que realizamos ARIMA , nuestro predicción se parece más a los datos reales por lo que podemos decir que el modelo ARIMA es mejor.
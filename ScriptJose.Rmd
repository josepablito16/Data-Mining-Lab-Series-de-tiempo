---
title: "Script"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tabulizer)
library(dplyr)
library(stringr)
library(fUnitRoots)
library(forecast)
library(lmtest)
```

## Obtener datos desde el PDF
```{r}
pages<-extract_tables("C01-Importación-de-combustibles-VOLUMEN-2020-03.pdf") #PDF con datos actualizados
datosImp <- do.call(rbind, pages)
nombresVar<-datosImp[1,]
datosImp<-as.data.frame(datosImp[2:nrow(datosImp),])
nombresVar[c(1,4,5,6,8,10,11,15,16,21,23,24)]<-c("Anio","GasAviacion","GasSuperior","GasRegular","rTurboJet","DieselLS","DieselULS","AceitesLub","GrasasLub","PetroleoReconst","Orimulsion","MezclasOleosas")
names(datosImp)<-nombresVar
```

```{r}
#Leer
dataSet = read.csv("datos.csv",stringsAsFactors = FALSE, na.strings = TRUE, strip.white = TRUE,sep = ",", encoding="UTF-8" )

```

```{r}
#Seleccionar
dataset = dataSet[c(1,2,5,6, 9,10)]
dataset = dataset[-c(46, 96, 146, 196),]
```

```{r}
#Limpiar
dataset$Diesel[dataset$Diesel =="-"] <- 0
dataset$DieselLS[dataset$DieselLS =="-"] <- 0
dataset2 = dataset
```

```{r}
#COnvertir
options(digits=9)
dataset2$Anio = as.numeric(dataset2$Anio)
dataset2$Mes = as.numeric(dataset2$Mes)
dataset2$GasSuperior = as.numeric(gsub(",", "", dataset2$GasSuperior))
dataset2$GasRegular = as.numeric(gsub(",", "", dataset2$GasRegular))
dataset2$Diesel = as.numeric(gsub(",", "", dataset2$Diesel))
dataset2$DieselLS = as.numeric(gsub(",", "", dataset2$DieselLS))
#Unir Diesel
dataset2$Diesel = dataset2$Diesel + dataset2$DieselLS
dataset2 = dataset2[-6]
dataSet = dataset2
View(dataSet)
```



## Serie de tiempo Diesel

Primero creamos la serie de tiempo para los datos del Diesel
```{r}
View(dataSet)
diesel <- ts(dataSet[dataSet$Diesel!="Diesel" & dataSet$Diesel!="-","Diesel"], start=c(2001, 1), end=c(2020,3), frequency=12)

class(diesel)
View(diesel)
```

## Construcción del modelo ARIMA

# Identificación 
A continuación se exploran las características de la serie: 

Frecuencia de la serie: 
```{r}
frequency(diesel)
```

Frecuencia de la serie: 

# Gráfica de la serie de tiempo
```{r}
plot(diesel)
abline(reg=lm(diesel~time(diesel)), col=c("red"))
```

Podemos observar que es una serie de tiempo continua con ligera tendencia a crecer por lo que es una serie de tiempo no estacionaria. 


# Descomposición de la serie
```{r}
plot(decompose(diesel))
```

Podemos observar una serie con tendencia a aumentar, que no es estacionaria en varianza y también tiene estacionalidad.


A continuación se separa la serie de tiempo en entrenamiento y prueba

```{r}
train <- head(diesel, round(length(diesel) * 0.7))
h <- length(diesel) - length(train)
test <- tail(diesel, h)
```

## Estimar los parámetros del modelo
A continuación aplicaremos una transformación logaritmica para hacer que la serie sea constante en varianza. 

```{r}
logTrain <- log(diesel)
plot(decompose(logTrain))
```

```{r}
plot(logTrain)
abline(reg=lm(logTrain~time(logTrain)), col=c("red"))
```

Podemos notar que se logro hacer un poco más constante la varianza de la serie. A continuación verificaremos que es estacionaria en media. Si tiene raices unitarias podemos decir que no es estacionaria en media. 

```{r}
adfTest(train)
```

```{r}
unitrootTest(train)
```

Como podemos notar, en ambas pruebas el valor de p es mayor a 0.05 por lo que no se puede rechazar la hipótesis nula de ausencia de raíces unitarias. Entonces por ende no es estacionaria en media.  

A coninuación hacemos lo mismo pero con una diferenciación:

```{r}
adfTest(diff(train))
```

```{r}
unitrootTest(diff(train))

```

Como podemos observar en esta ocasión el valor de p está por debajo de 0.05 por lo que ahora si se puede descartar la hipótesis nula de que existen raíces unitarias. Podemos notar entonces que solo es necesaria una diferenciación (d=1). 

El siguiente paso es intentar identificar los parámetros p y q usando los gráficos de autocorrelación y autocorrelación parcial.

```{r}
acf(logTrain,50) 
```

En el gráfico de la función podemos notar que se anula después del tercer retardo por lo que se puede sugerir q=3.

```{r}
pacf(logTrain,20) 
```

En el gráfico de la función podemos notar que se anula después del retardo 0 por lo que podríamos sugerir un coeficiente p=0.

Podemos sugerir un modelo con los siguientes parámetros: 

* p=0.
* q=3.
* d=1.

O en otras palabras ARIMA(p,d,q).

Volvamos a ver la descomposición de la serie paran determinar si hay estacionalidad 

```{r}
decTrain <- decompose(logTrain)
plot(decTrain$seasonal)
```

Podemos notar que si existe estacionalidad en la serie. A continuación, se buscarán los parámetros estacionales usando las funciones de autocorrelación y autocorrelación parcial. 

```{r}
Acf(diff(logTrain),84)
```

Podemos notar que en casi todos los períodos hay dos decaimientos estacionales por lo que podemos sugerir P=2.

```{r}
Pacf(diff(logTrain),84)
```

Podemos darnos cuenta que los pico significativos están al inicio de la gráfica por lo que podemos sugerir D=1. Finalmente Q=0.

Con los parámetros determinados generamos un modelo: 

```{r}
fitArima <- arima(logTrain,order=c(0,1,3),seasonal = c(2,1,0))
```

R tiene la capacidad de generar un modelo automáticamente, entonces también lo tomaremos en cuenta: 

```{r}
fitAutoArima <- auto.arima(train)
```


## Significación de los coeficientes: 

A continuación, analizaremos que tan significativos son los coeficientes de cada modelo

```{r}
coeftest(fitArima)
```

Podemos notar que todos los coeficientes son significativo ya que todos son menores a 0.05.

```{r}
coeftest(fitAutoArima)
```

En este caso podemos notar no todos los coeficientes son significativos. 

## Análisis de Residuales

A continuación, se analizarán los residuales de ambos modelos. Estos deben estar distribuidos normalmente y se deben de parecer al ruido blanco.

```{r}
qqnorm(fitArima$residuals)
qqline(fitArima$residuals)
```

```{r}
checkresiduals(fitArima)
```

Podemos notar que los datos tienen una distribución normal. Sin embargo, según el test de Ljung-Box los datos se distribuyen de forma dependiente puesto que el p-value es menor a 0.05 y se puede rechazar la hipótesis nula. Esto quiere decir que el modelo no es aceptable para predecir.

Analicemos también el modelo generado automáticamente por R: 

```{r}
qqnorm(fitAutoArima$residuals)
qqline(fitAutoArima$residuals)
```
```{r}
checkresiduals(fitAutoArima)
```

En este caso podemos notar que el test Ljung-Box dice que los datos se distribuyen de forma independiente puesto que el p-value es mayor a 0.05, por lo tanto, puede rechazarse la hipótesis nula y el modelo es aceptable para predecir. 


## Predicción con el modelo generado

Pasaremos a predecir con la función forecast la misma cantidad de meses que hay en test y compararemos con los valores reales. 

```{r}
fitArima <- arima(log(train),c(0,1,3),seasonal =list( order=c(2,1,0),period=12))
test2<-ts(test,start = c(2014,7),end = c(2020,3),frequency = 12)
fitArima %>%
  forecast(h) %>%
  autoplot() + autolayer(log(test2))
```

Podemos notar que nuestro modelo es bueno prediciendo, ya que este se parece bastante a los datos reales, así como también está dentro del intervalo de confianza.

```{r}
fitAutoArima <- auto.arima(train)
test3<-ts(test,start = c(2014,7),end = c(2020,3),frequency = 12)
fitAutoArima %>%
  forecast(h) %>%
  autoplot() + autolayer(test3)
```

Podemos notar que el modelo realizado automáticamente por R no predice muy bien, ya que no se parece en nada a los datos reales. Entonces podemos decir que nosotros hemos construido un mejor modelo al que genera automáticamente R. 